\documentclass[onecolumn, 12pt, a4paper]{article}
\usepackage{physics}
 
\author{
  George Herbert\\
  \texttt{cj19328@bristol.ac.uk}
}

\title{An Unknown Signal Report}
\begin{document}

\maketitle

\begin{abstract}
    This report demonstrates my understanding of the methods I have 
    used, the results results I have obtained and my understanding
    of issues such as overfitting for the `An Unknown Signal'
    coursework.
\end{abstract}

\section{Equations for linear regression}

For a set of points that lie along a line with Gaussian noise 
$\vb{y} = \vb{X}\vb{w} + \vb{\epsilon}$ where $\epsilon_{i} \sim \mathcal{N}(0, \sigma^{2})$,
the maximum likelihood esimation is equivalent to the least square 
error estimation and is given by the equation:
\[
    \vb{\hat{w}} = (\vb{X}^{T}\vb{X})^{-1}\vb{X}^{T}\vb{y}.
\]
I've implemented this equation in my code as the following function:
\begin{verbatim}
def regressionNormalEquation(self, X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y
\end{verbatim}

\section{Choice of polynomial order}

\section{Choice of unknown function}

\section{Overfitting}

Overfitting occurs when a machine learning algorithm
produces a model that has learnt the noise in the data
as if it represents the structure of the underlying
model. \cite{MSMI}


When producing a fit for a given line segment,
overfitting is most likely to occur the algorithm 
produces a fit that has
learnt the noise in the data as if it represents the
structure of the underlying model.

In the case of linear regeression, overfitting typically occurs
when the model produced contains too complex a function class,
such that it would fail to predict future observations.

\section{Model selection}

To prevent overfitting each line segment, I have used a type of 
cross-validation known as $k$-fold cross-validation---in particular,
I have used 5-fold cross-validation. 
I opted to use $k$-fold cross-validation due to the small number
of datapoints in each segment.

To begin with, $k$-fold cross-validation involves shuffling and 
splitting each 20-point segment into $k$ evenly-sized subsamples,
as implemented in the \texttt{getKFold()} method.
Then, each subsample is used exactly once as validation data,
whilst the other $k - 1$ paritions are used as training data.
The cross-validation error for each model is calculated
as follows \cite{EOSL}:
\[
    CV({\hat{f}}) = \frac{1}{N}\sum_{i = 1}^{N}SSE(y_{i}, \hat{f}_{i}(x_{i}))
\]
where
$\hat{f}$ is a given model;
$\hat{f}_{i}$ is the fitted function, trained with the $i$-th subsample removed;
$N$ is the number of folds (in this case 5);
$SSE$ is the sum squared error function;
$y_{i}$ is the y datapoints of subsample $i$;
and $x_{i}$ is the x datapoints of subsample $i$.

The model with the lowest cross-validation error is then selected.

\section{Testing}

\section{Optimisations and improvements}

\begin{enumerate}
    \item{Using \texttt{np.linalg.solve}}
    \item{Not shuffling the data before leave-one-out cross-validation}
    \item{Not finding the average cross-validation error}
\end{enumerate}

\begin{thebibliography}{9}

    \bibitem{MSMI}
    Burnham, K. P. and Anderson, D. R. (2002)
    \textit{Model Selection and Multimodel Inference}.
    2nd ed. Springer-Verlag.

    \bibitem{EOSL} 
    Hastie, T., Tibshirani, R. and Friedman, J. (2001)
    \textit{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}. 
    2nd ed. New York: Springer.

\end{thebibliography}
    

\end{document}
